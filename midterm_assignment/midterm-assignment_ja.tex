\documentclass{jsarticle}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{ART.T458 Advanced Machine Learning Midterm Assignment
(Final report assigned by Shimosaka)}

\author{Masamichi Shimosaka
Department of Computer Science Tokyo Institute of Technology}

\begin{document}
\maketitle
\section*{Introduction}
\begin{itemize}
  \item 好みの問題を選び、選ばれた問題を解く。
  \item Tex/MSwordを使用.
  \item 2021年7月30日17:00までに、A4サイズ／レターサイズのレポート（pdf形式のファイル）をt2scholaで提出してください。
  \item 各問題で獲得できる最大ポイントは、各セクションのタイトルに表示されています。
  \item i番目の問題で獲得したスコアを$s_i \ge 0$とすると、評価に使用する最終スコア$q$は次のように処理されます。
$$q = \min(30,\Sigma_i s_i)$$
  \item 上記のqptに加えて、本講座の前半で履修したい機械学習のトピックを、線形モデル、スパース学習、最適化、後半の内容（深層学習）と同様に示していただければ、2pt（qとは無関係）を獲得できます。アンサンブルモデル（決定付きブースティング、回帰トレス、バギングなど）、ベイズ法（ノンパラメトリックベイズ、マルコフ連鎖モンテカルロ法、変分ベイズなど）、シーケンスモデリング（カルマンフィルタ、マルコフモデル、CRFなど）、応用的な観点からの課題（レコメンダーシステム（ランキング学習、因子化マシン）など）、数値最適化（内包点アルゴリズム、拡張ラグランジアン、ADMM、逐次二次計画など）など、お好みのキーワードを取り上げていただければと思っています。前半・後半の講義の内容を参考にすることはご遠慮ください。また，スライドに誤字・脱字があった場合は，「x 回目の講義のスライド y ページ目の z の式に誤りがあり，w のように修正すべきです」という提案を，ボーナスとして 2pt 追加することができます。個人的には，コースの質を向上させるための提案も，もちろん歓迎します。
  \item 報告書には、実装のソースコードを記載する必要はありません。ただし、githubなどの公開されているリポジトリサービスを利用して、コードのURLを示すことは歓迎されます。
  \item 重要：本レポートでは，SciPy, scikit-learn, (Py)-Torch, Tensor Flow, MatlabのNeural network toolboxなどの高度な機械学習ライブラリを使用することはできませんが，NumPyなどの基本的な線形代数ライブラリと基本的なMatlab言語機能を使用することができます．なお、本報告書の主な目的は、ブラックボックス化したMLライブラリの使い方ではなく、実装によるMLの数学的観点を理解することにあります。
  \item 英語だけでなく、日本語も使えます。
  \item https://bit.ly/32gbpRt のJupyter notebookにあるいくつかの参考コードは、この課題を推進するのに役立つかもしれませんし、また頻繁に更新されるかもしれません。このスクリプトを使うのではなく、最初から自分でコードを作っても構いません。もちろん、Pythonだけでなく、Matlabも使用可能です。

  注：*、**、***は、それぞれ各問題の難易度を示しています。と表示されている問題は、ほとんど解けると思います。機械学習の数学的な側面を強化したい場合は、**, ***を選択することをお勧めします。なお，MLソフトを実装していなくても30ptを獲得することができます :-)．
\end{itemize}

\section*{Problem 1 (10pts)*}
ここでは、線形ロジスティック回帰による二値分類を考える。$x \in \mathbb R_d$をd次元の入力ベクトル、$w \in \mathbb R_d$をモデルのパラメータとします。
分類器は，$f(x) = 2 [\![w^\top x]\!] > 0 - 1$で表され，$c$は，$c$が真であれば1を，そうでなければ0を返す指示関数を表します．この最適化問題は次のように書くことができます。
教師付きデータセット${\bm x_i, y_i}^n_{i=1}$を用いて，ロジスティック回帰の最適化問題を考えます．この最適化問題は次のように書くことができます。
\begin{align*}
  \hat{\bm w} &=  \argmin_{\bm w} J(\bm w) \\
  J(\bm w) &:= \sum_{i=1}^n (\ln (1+\exp(-y_i\bm w^\top \bm x_i))) + \lambda\bm w^\top\bm w.
\end{align*}
ここでは，$d - 1$ 次元の特徴空間でのオフセットに分類器を適応させるために，$\bm x_i$ に定数値 1 が含まれていると仮定する．
いくつかの人工的なデータセット（Toy Datasetの項、Dataset IVを参照）を用いて、以下のようにいくつかの最適化手法を実装することを考えます。

\begin{enumerate}
  \item バッチ式最優先勾配法$^1$の導入.
  \item ニュートンベースの手法を導入。
  \item 上記2つの最適化手法の性能を比較するために、$|J(\bm w^{(t)}) - J(\hat{\bm w})|$ w.r.t. $t$ (ここで、$\bm w^{(t)}$は、$t$番目の繰り返しにおけるパラメータを表し、$\hat{\bm w}$は、2つの方法で得られた$J$の最小値に達する最適なパラメータを表します)を半対数プロットで表します。
  \item マルチクラス版ロジスティック回帰（ToyデータセットVを使用）にニュートン法と単純な最勾配法を導入し、上記のバイナリーロジスティック回帰と同じ実験を行う。
\end{enumerate}

\section*{Problem 2 (5 pts)*}
我々は、線形回帰に平方損失とL1正則化を採用したlassoを考える。この問題では、近似勾配法（PG）を採用します。議論を簡単にするために、以下の目的を使用する。
\begin{align*}
  \hat{\bm w} = \argmin_{w} \left((\bm w-\bm\mu)\top \bm A(\bm w-\bm\mu)+\lambda||\bm w||_1\right).
\end{align*}
lassoのPGを実装し、いくつかの条件で結果を示してください。
この問題では、同じ学習率$\eta_t = L_{-1}$を使用します。ここで、$L$は目的の勾配のリプシッツ定数を表し、ヘシアン行列$2\bm A$から導かれます（つまり、学習率の逆数として$2\bm A$の最大固有値を使用します：$\eta_t^{-1}-1$）。
\begin{enumerate}
  \item PGの結果を、繰り返し回数に応じて$||\bm w^{(t)} − \hat{\bm w}||$で示す。セミログプロットを使用。次の条件を使用します。
  $$A=\begin{pmatrix}
  3& 0.5 \\
  0.5& 1
  \end{pmatrix}, \mu^\top=(1 2).$$
\end{enumerate}
L1正則化の特性を確認するために、$\lambda = 2, 4, 6$で実験を行います。数値結果は、講義で使用したスライドに記載されていることを思い出してください。また、cvx (matlab) / cvxopt (python) を使って結果を確認することもできます。

\section*{Problem 3 (15 pts)*}
ここでは，サポート・ベクトル・マシンの双対（L2正則化されたヒンジ・ロスに基づくバイナリ分類器）を考える。この分類の元々の最適化問題は，次のように表すことができる。
\begin{align}
  \hat{\bm w} = \argmin_{\bm w\in \mathbb R^d}\left(\sum_{i=1}^n\max(0, 1 − y_i\bm w^\top \bm x_i) + \lambda||\bm w||_2^2\right),
\end{align}
ここで $\bm x_i \in \mathbb R^d$, $\bm w \in \mathbb R^d$, $y_i \in {±1}$, $\lambda > 0$はそれぞれ$i$番目の入力変数、パラメータベクトル、$i$番目の入力データのラベル、正則化項の係数を表す。
\begin{enumerate}
  \item この最適化の双対ラグランジュ関数が次のように書けることを検証する。
  \begin{align}
    \nonumber
    \mathrm{maximize}_{ \bm\alpha\in\mathbb R_n}&\ − \frac{1}{4\lambda} \bm\alpha^\top\bm K\bm\alpha + \bm\alpha^\top\bm 1,  \\
    \mathrm{subject\ to}&\ \bm 0\le\bm\alpha\le\bm 1
  \end{align}
  ここで、$\bm 1$, $\bm 0$は、それぞれ$1$, $0$を要素とするn次元のベクトルを表し、$K \in \mathbb R^{n×n}$は対称正方行列を表し、その$i$番目の行と$j$番目の列の要素は$y_iy_j\bm x_i^\top\bm x_j$である。
  \item KKT条件から、$\alpha$で与えられる最適な重みパラメータ$\bm w$が次のように書けることを検証する。
  \begin{align}
    \hat{\bm w} = \frac{1}{2\lambda}\sum_{i=1}^n \alpha_iy_i\bm x_i
  \end{align}
  \item 負の双対ラグランジュ関数の最小化を、投影勾配を用いて実施する。妥当性については、双対ラグランジュ関数のスコア（(2)を使用）と、ヒンジ損失関数と正則化の和を、それぞれ反復回数（(1)と(3)を使用）に対して示す。双対性ギャップが0になると収束することを確認する。ここでは、投影勾配がちょうど計算されると仮定する。
  $$
  \bm\alpha^{(t)} = P_{[0,1]^n} \left(\bm\alpha^{(t−1)} − \eta_t\left(\frac{1}{2\lambda}\bm K\bm\alpha^{(t−1)} − \bm 1\right)\right),
  $$
  ここで、$\eta_t$は、$t$番目の反復における学習率を表し、$P_{[0,1]^n}$は、入力された各キャストを$[0,1]$に投影する投影演算子を表している。
\end{enumerate}

\section*{Problem 4 (10 pts)**}
ここでは、ヒンジ損失関数とL1正則化を用いた二値分類問題を考える。また、$bm x \mathbb R^d$を入力、$bm w \mathbb R^d$をモデルのパラメータとする。
ここでは、線形回帰による二値判別関数を$f(x) = 2[\![w^\top x \ge 0]\!] − 1$とし、$[\![c]\!]$は、$c$が真であれば1を返し、そうでなければ0を返すものとします。
\begin{align}
  \hat{\bm w} = \argmax_{\bm w\in \mathbb R^d}(0, 1 − y_i\bm w^\top x_i) + \lambda||\bm w||_2^2,
\end{align}
ここで、$y_i \ in {\pm1}$ は、$i$番目の学習例のバイナリラベルを表します。
\begin{enumerate}
  \item (4)から線形プログラムを導出する（補助変数 $\xi_i \ge \max(0, 1 - y_i\bm w^\top \bm x_i) \ge 0$, $e_i \ge |\bm w_i| \ge 0$）。線形プログラムは次のように書けることを思い出してください。
  \begin{align*}
  \mathrm{minimize}&\ \bm c^\top \bm z \\
  \mathrm{subject to}&\ \bm A\bm z \le \bm b
  \end{align*}
  (L1正則化ヒンジロスモデルからのLPを扱うLpBoostを参照）．)
  \item いくつかの人工的なデータセット（ToyDatasetsection, DatasetIV 参照）を用いて， cvx (in Matlab) / cvxopt (in python) (参考)と，（バッチ式の）近位劣位法によって，この問題を実装してください．そして，パラメータが適切に収束することを確認します．データセットの仕様は、報告書に記載する必要があります。
\end{enumerate}

\section*{Problem 5 (10 pts + optional 5 pts)**}
以下の目的を持つ行列最適化問題を考えます。
\begin{align*}
  \argmin_{\bm Z\in\mathbb R^{x\times n}} \left( \sum_{i,j\notin Q} |Ai,j − Zi,j |2 + λ||\bm Z||_*\right)
\end{align*}
ここで $\bm A \in \mathbb R^{m\times n}$ はデータ行列（推薦システムにおけるユーザー対映画の評価データ）を表し、$Q$ にはヌル値も含まれます。
また、$||・|_*$は行列の核ノルムを表し、$\lambda > 0$は正則化のためのハイパーパラメータを表します。
この最適化問題では、$\bm Z$は不完全なデータ行列$\bm A$から復元されたデータに相当します。推薦システムのシナリオでは、推定された$\bm Z$の位置$Q$は、映画の評価の推定スコアに相当します。
\begin{enumerate}
  \item 行列の核ノルムの定義をwwwから調べて記述しなさい。その定義に加えて、核ノルムを用いた近位演算を定義しなさい。(ヒント：特異値分解を使う)
  \item いくつかのデータセット（Toy Dataset III参照）を用いて、この機械学習問題に近位勾配法を実装し、復元されたデータ$\bm Z$を曲面プロットで示します。
  \item (Option: Additional 5 pts獲得可能) $\bm A$を復元するための代替アプローチとして非負行列因子分解を実装し、ハイパーパラメータを選択してパフォーマンスを比較する。ここで実装した2つの方法の長所と短所を説明してください。
\end{enumerate}

\section*{Problem 6 (10 pts)***}
$L$-Lipschitzの凸関数$f$を最小化するための勾配降下法を考える。開始点から $\bm w^{(0)} \in \mathbb R_d$を出発点とし、各反復において、パラメータ$\bm w$を、各反復において、パラメータ$\bm w$を$\bm w^{(t+1)} \leftarrow \bm w^{(t)} − \eta \nabla f|_{\bm w=\bm w^{(t)}}$のように更新する（固定ステップサイズの勾配降下法による更新）。最後に、最適な値を選択します。
\begin{align*}
  \hat{\bm w} = \argmin_{\bm w^{(0)},\bm w^{(1)},\ldots,\bm w^{(n)},} \{f(\bm w^{(0)}),f(\bm w^{(1)}),\ldots,f(\bm w^{(T)})\}
\end{align*}
. ここでは、関数 $f$ を最小化するための最適な値を $f(\hat{\bm w})−f(\bm w^*) \le \varepsilon$ としたときに成立する最小の反復回数 $T^*$ を求めます。ここでは、関数$f$を最小化するための最適値を$\bm w^*$とします。このような$T^*$は、$O(1/\varepsilon^2)$と書けることを証明してください。

\section*{Problem 7 (10 pts)***}
問題6と同様に, 滑らかな凸関数$f$を最小化するための勾配降下法を考えます。開始点から $\bm w^{(0)} \in \mathbb R_d$としてパラメータを更新していく。$\bm w^{(t+1)} \leftarrow \bm w^{(t)} − \eta \nabla f|_{\bm w=\bm w^{(t)}}$としてパラメータ$\bm w$を更新します。 - と更新する（固定ステップサイズの勾配降下法）。最後に、最適な値を選択します。
$$
\hat{\bm w} = \argmin_{\bm w^{(0)},\bm w^{(1)},\ldots,\bm w^{(n)},} \{f(\bm w^{(0)}),f(\bm w^{(1)}),\ldots,f(\bm w^{(T)})\}
$$
. ここでは、$\eta = 1/\gamma$を設定したときに$f(\hat{\bm w})−f(\bm w_*) \le \varepsilon$が成立する最小の反復回数$T^*$を推定したい。ここでは、関数$f$を最小化するための最適値を$\bm w^*$とします。このような$T^*$は、$O(1/\epsilon)$と書けることを証明してください。

\section*{Problem 8 (10pts)*}
線形回帰と正則化の効果について考えます。講義で示したように、最小二乗問題として知られる線形回帰の最適化は、以下の最適化問題として定義されます。
$$
\hat{\bm w}_{LS} = \argmin_{\bm w} \frac{1}{2}||\bm y − \bm X\bm w||_2^2,
$$
ここで、設計行列、応答ベクトル、パラメータはそれぞれ$\bm X \in \mathbb R^{n×d}$, $\bm y \in \mathbb R^n$，and $\bm w \in \mathbb R^d$で表されます（必要に応じて、講義のスライドを確認し、ビデオをチェックしてください）。この最適化による回帰は，条件によっては正常に動作することもありますが，オーバーフィッティングが起こりやすいことも知られています。オーバーフィッティングの問題に対処する簡単な方法として、リッジ正則化は機械学習の分野でよく使われています。その結果、最適化問題は次のように定義されます。
$$
\hat{\bm w}_{ridge} = \argmin_{\bm w} \frac{1}{2}||\bm y − \bm X\bm w||_2^2 + \lambda||\bm w||_2^2.
$$
\begin{enumerate}
  \item それぞれ、$\hat{\bm w}_{LS}$、$\hat{\bm w}_{ridge}$の解析解を得る。
  ここでは、$\bm X^\top \bm X$が正則であること、すなわち、$(\bm X^\top \bm X)−1$が存在することを仮定する。
  \item たとえ$\bm X^\top \bm X$が規則的でなくても、$\bm X^\top \bm X + \lambda \bm I$が規則的であることを証明してください。つまり、$\bm w_{ridge}$の最適なパラメータは、$\bm X^\top \ bm X$が正則であるかどうかに関わらず、常に利用できるということです。ここで、$\bm I \in \mathbb R^{d×d}$は、恒等行列を表し、$\lambda > 0$は、正則化のハイパーパラメータを表します。
  \item ここでは、$\bm X^top \bm X$が規則的であると仮定して、$||\bm X\hat{\bm w}_{LS}||_2^2 \ge ||\bm X\hat{\bm w}_{ridge}||_2^2$を証明します。この結果は、機械学習におけるシュリンケージとも呼ばれています。機械学習において、縮退が有効に働く場面を説明してください。
\end{enumerate}
　
\section*{Problem 9 (10 pts) **}
\begin{enumerate}
  \item ここでは、$(x,y)$から点$\{x_i,y_i\}^n_{i=1}$までの最大距離を最小化することを考えます。
  $$
  \mathrm{minimize}_{x\in\mathbb R,y\in\mathbb R}\quad\mathrm{max}_i\sqrt{(x_i-x)^2+(y_i-y)^2}
  $$
  この最適化問題を二次計画問題に変換します。二次関数を目的関数として使用し、制約条件には線形の等式と不等式を使用します。
  \item 次の最適化を凸最適化に変換してください。ここでは、変数$x_1$, $x_2$, $x_3$をそれぞれ実数とする。ヒント：4番目の制約条件に注目し、$x_1$, $x_2$, $x_3$をそれぞれ他の変数で表してください。
  \begin{align}
    \nonumber
    \mathrm{minimize}_{x_1,x_2,x_3}&\ x_2/x_1 \\
    \nonumber
    \mathrm{subject\ to}&\ x_1^2+x_2/x_3 \le \sqrt x_2 \\
    \nonumber
    &\ x_1/x_2 = x_3^2 \\
    \nonumber
    &\ 2 \le x_1 \le3, \\
    \nonumber
    &\ x_1,x_2,x_3 >0 
  \end{align}
\end{enumerate}

　　
\end{document}
